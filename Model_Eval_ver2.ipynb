{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda3/envs/nlpclass/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adjusted_BoxOffice</th>\n",
       "      <th>BoxOffice</th>\n",
       "      <th>NLP_Score</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Year</th>\n",
       "      <th>imdbRating</th>\n",
       "      <th>imdbVotes</th>\n",
       "      <th>Internet_Movie_Database</th>\n",
       "      <th>Rotten_Tomatoes</th>\n",
       "      <th>Metacritic</th>\n",
       "      <th>...</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>News</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Short</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "      <th>Country_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9322.000000</td>\n",
       "      <td>9322.000000</td>\n",
       "      <td>9322.000000</td>\n",
       "      <td>9322.000000</td>\n",
       "      <td>9322.000000</td>\n",
       "      <td>9322.000000</td>\n",
       "      <td>9.322000e+03</td>\n",
       "      <td>9322.000000</td>\n",
       "      <td>9322.000000</td>\n",
       "      <td>9322.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9322.000000</td>\n",
       "      <td>9322.000000</td>\n",
       "      <td>9322.000000</td>\n",
       "      <td>9322.000000</td>\n",
       "      <td>9322.0</td>\n",
       "      <td>9322.000000</td>\n",
       "      <td>9322.000000</td>\n",
       "      <td>9322.000000</td>\n",
       "      <td>9322.000000</td>\n",
       "      <td>9322.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>28.648639</td>\n",
       "      <td>21.948306</td>\n",
       "      <td>9.158985</td>\n",
       "      <td>104.624973</td>\n",
       "      <td>2005.691697</td>\n",
       "      <td>6.378696</td>\n",
       "      <td>5.093996e+04</td>\n",
       "      <td>6.378696</td>\n",
       "      <td>5.706821</td>\n",
       "      <td>5.648604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078631</td>\n",
       "      <td>0.002253</td>\n",
       "      <td>0.225595</td>\n",
       "      <td>0.064578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031216</td>\n",
       "      <td>0.204784</td>\n",
       "      <td>0.030573</td>\n",
       "      <td>0.011049</td>\n",
       "      <td>1.535078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>64.164851</td>\n",
       "      <td>51.199261</td>\n",
       "      <td>5.261931</td>\n",
       "      <td>17.940863</td>\n",
       "      <td>7.626457</td>\n",
       "      <td>1.004886</td>\n",
       "      <td>1.191995e+05</td>\n",
       "      <td>1.004886</td>\n",
       "      <td>2.639098</td>\n",
       "      <td>1.517948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269177</td>\n",
       "      <td>0.047412</td>\n",
       "      <td>0.417996</td>\n",
       "      <td>0.245794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173912</td>\n",
       "      <td>0.403566</td>\n",
       "      <td>0.172167</td>\n",
       "      <td>0.104538</td>\n",
       "      <td>0.974243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>1.046110</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>1990.000000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082429</td>\n",
       "      <td>0.066174</td>\n",
       "      <td>4.624634</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>2.052000e+03</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.657188</td>\n",
       "      <td>1.250904</td>\n",
       "      <td>8.212019</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>9.069500e+03</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>5.940134</td>\n",
       "      <td>5.756893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>29.015950</td>\n",
       "      <td>21.010307</td>\n",
       "      <td>12.778660</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>4.437925e+04</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1039.048028</td>\n",
       "      <td>936.658640</td>\n",
       "      <td>28.108077</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>2018.000000</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>2.018482e+06</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Adjusted_BoxOffice    BoxOffice    NLP_Score      Runtime         Year  \\\n",
       "count         9322.000000  9322.000000  9322.000000  9322.000000  9322.000000   \n",
       "mean            28.648639    21.948306     9.158985   104.624973  2005.691697   \n",
       "std             64.164851    51.199261     5.261931    17.940863     7.626457   \n",
       "min              0.000082     0.000074     1.046110    53.000000  1990.000000   \n",
       "25%              0.082429     0.066174     4.624634    92.000000  2000.000000   \n",
       "50%              1.657188     1.250904     8.212019   101.000000  2007.000000   \n",
       "75%             29.015950    21.010307    12.778660   113.000000  2012.000000   \n",
       "max           1039.048028   936.658640    28.108077   197.000000  2018.000000   \n",
       "\n",
       "        imdbRating     imdbVotes  Internet_Movie_Database  Rotten_Tomatoes  \\\n",
       "count  9322.000000  9.322000e+03              9322.000000      9322.000000   \n",
       "mean      6.378696  5.093996e+04                 6.378696         5.706821   \n",
       "std       1.004886  1.191995e+05                 1.004886         2.639098   \n",
       "min       1.100000  6.000000e+00                 1.100000         0.000000   \n",
       "25%       5.800000  2.052000e+03                 5.800000         3.700000   \n",
       "50%       6.500000  9.069500e+03                 6.500000         5.940134   \n",
       "75%       7.100000  4.437925e+04                 7.100000         8.000000   \n",
       "max       9.300000  2.018482e+06                 9.300000        10.000000   \n",
       "\n",
       "        Metacritic      ...            Mystery         News      Romance  \\\n",
       "count  9322.000000      ...        9322.000000  9322.000000  9322.000000   \n",
       "mean      5.648604      ...           0.078631     0.002253     0.225595   \n",
       "std       1.517948      ...           0.269177     0.047412     0.417996   \n",
       "min       0.100000      ...           0.000000     0.000000     0.000000   \n",
       "25%       4.800000      ...           0.000000     0.000000     0.000000   \n",
       "50%       5.756893      ...           0.000000     0.000000     0.000000   \n",
       "75%       6.500000      ...           0.000000     0.000000     0.000000   \n",
       "max      10.000000      ...           1.000000     1.000000     1.000000   \n",
       "\n",
       "            Sci-Fi   Short        Sport     Thriller          War  \\\n",
       "count  9322.000000  9322.0  9322.000000  9322.000000  9322.000000   \n",
       "mean      0.064578     0.0     0.031216     0.204784     0.030573   \n",
       "std       0.245794     0.0     0.173912     0.403566     0.172167   \n",
       "min       0.000000     0.0     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.0     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.0     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.0     0.000000     0.000000     0.000000   \n",
       "max       1.000000     0.0     1.000000     1.000000     1.000000   \n",
       "\n",
       "           Western  Country_count  \n",
       "count  9322.000000    9322.000000  \n",
       "mean      0.011049       1.535078  \n",
       "std       0.104538       0.974243  \n",
       "min       0.000000       1.000000  \n",
       "25%       0.000000       1.000000  \n",
       "50%       0.000000       1.000000  \n",
       "75%       0.000000       2.000000  \n",
       "max       1.000000      16.000000  \n",
       "\n",
       "[8 rows x 35 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data_combined.csv')\n",
    "data = data[(data.Runtime >= 50) & (data.Runtime <= 200) & \n",
    "            (data.Year >= 1990)]\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1084a37b8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# transfer to category\n",
    "log_boxoffice = np.log(data['Adjusted_BoxOffice'])\n",
    "cat_raw = pd.cut(log_boxoffice, [-10,-6,-4,-2,0,2,4,6,10])\n",
    "all_cats = cat_raw.unique()\n",
    "cat_encoder = dict([(i,x) for x,i in enumerate(all_cats)])\n",
    "data['Adjusted_BoxOffice_Cat'] = cat_raw.replace(cat_encoder)\n",
    "data['Adjusted_BoxOffice_Cat'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model1: CV Accuracy: 31.663 | Testing Accuracy: 31.641\n",
      "Baseline Model2: CV Accuracy: 40.245 | Testing Accuracy: 41.974\n"
     ]
    }
   ],
   "source": [
    "#Baseline Model: A simple logistic regression\n",
    "\n",
    "data_model1 = data[['Adjusted_BoxOffice_Cat', 'Runtime',\n",
    "        'Action','Adventure', 'Animation', 'Biography', 'Comedy', 'Crime', 'Documentary',\n",
    "       'Drama', 'Family', 'Fantasy', 'Film-Noir', 'History', 'Horror', 'Music',\n",
    "       'Musical', 'Mystery', 'News', 'Romance', 'Sci-Fi', 'Short', 'Sport',\n",
    "       'Thriller', 'War', 'Western', 'Country_count']]\n",
    "\n",
    "X = data_model1.drop('Adjusted_BoxOffice_Cat',axis = 1)\n",
    "y = data_model1['Adjusted_BoxOffice_Cat']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state = 1)\n",
    "\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "scores = cross_val_score(lr_clf, X_train, y_train, cv=5)\n",
    "\n",
    "lr_clf.fit(X_train, y_train)\n",
    "y_pred = lr_clf.predict(X_test)\n",
    "tst_acc = accuracy_score(y_test, y_pred) * 100\n",
    "cv_acc = np.mean(scores) * 100\n",
    "print('Baseline Model1: CV Accuracy: %0.3f | Testing Accuracy: %0.3f' % (cv_acc,tst_acc))\n",
    "\n",
    "\n",
    "data_model2 = data[['Adjusted_BoxOffice_Cat', 'Runtime','Year', 'imdbRating', 'imdbVotes',\n",
    "       'Internet_Movie_Database', 'Rotten_Tomatoes', 'Metacritic',\n",
    "       'Action','Adventure', 'Animation', 'Biography', 'Comedy', 'Crime', 'Documentary',\n",
    "       'Drama', 'Family', 'Fantasy', 'Film-Noir', 'History', 'Horror', 'Music',\n",
    "       'Musical', 'Mystery', 'News', 'Romance', 'Sci-Fi', 'Short', 'Sport',\n",
    "       'Thriller', 'War', 'Western', 'Country_count']]\n",
    "\n",
    "X = data_model2.drop('Adjusted_BoxOffice_Cat',axis = 1)\n",
    "y = data_model2['Adjusted_BoxOffice_Cat']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state = 1)\n",
    "\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "scores = cross_val_score(lr_clf, X_train, y_train, cv=5)\n",
    "\n",
    "lr_clf.fit(X_train, y_train)\n",
    "y_pred = lr_clf.predict(X_test)\n",
    "tst_acc = accuracy_score(y_test, y_pred) * 100\n",
    "cv_acc = np.mean(scores) * 100\n",
    "print('Baseline Model2: CV Accuracy: %0.3f | Testing Accuracy: %0.3f' % (cv_acc,tst_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR(NLP): CV Accuracy: 35.554 | Testing Accuracy: 36.539\n"
     ]
    }
   ],
   "source": [
    "# LR with NLP\n",
    "data_model3 = data[['Adjusted_BoxOffice_Cat', 'Runtime',\n",
    "        'Action','Adventure', 'Animation', 'Biography', 'Comedy', 'Crime', 'Documentary',\n",
    "       'Drama', 'Family', 'Fantasy', 'Film-Noir', 'History', 'Horror', 'Music',\n",
    "       'Musical', 'Mystery', 'News', 'Romance', 'Sci-Fi', 'Short', 'Sport',\n",
    "       'Thriller', 'War', 'Western', 'Country_count','NLP_Score']]\n",
    "\n",
    "X = data_model3.drop('Adjusted_BoxOffice_Cat',axis = 1)\n",
    "y = data_model3['Adjusted_BoxOffice_Cat']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state = 1)\n",
    "\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "scores = cross_val_score(lr_clf, X_train, y_train, cv=5)\n",
    "\n",
    "lr_clf.fit(X_train, y_train)\n",
    "y_pred = lr_clf.predict(X_test)\n",
    "tst_acc = accuracy_score(y_test, y_pred) * 100\n",
    "cv_acc = np.mean(scores) * 100\n",
    "print('LR(NLP): CV Accuracy: %0.3f | Testing Accuracy: %0.3f' % (cv_acc,tst_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: CV Accuracy: 34.682 | Testing Accuracy: 35.967\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "data_model4 = data_model3\n",
    "X = data_model4.drop('Adjusted_BoxOffice_Cat',axis = 1)\n",
    "y = data_model4['Adjusted_BoxOffice_Cat']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state = 1)\n",
    "\n",
    "\n",
    "svm_clf = svm.SVC(decision_function_shape='ovo')\n",
    "scores = cross_val_score(svm_clf, X_train, y_train, cv=5)\n",
    "\n",
    "svm_clf.fit(X_train, y_train)\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "tst_acc = accuracy_score(y_test, y_pred) * 100\n",
    "cv_acc = np.mean(scores) * 100\n",
    "print('SVM: CV Accuracy: %0.3f | Testing Accuracy: %0.3f' % (cv_acc,tst_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN: CV Accuracy: 26.037 | Testing Accuracy: 26.707\n",
      "KNN: CV Accuracy: 31.603 | Testing Accuracy: 33.214\n",
      "KNN: CV Accuracy: 31.955 | Testing Accuracy: 33.035\n",
      "KNN: CV Accuracy: 31.542 | Testing Accuracy: 32.320\n",
      "KNN: CV Accuracy: 31.081 | Testing Accuracy: 32.142\n",
      "KNN: CV Accuracy: 30.791 | Testing Accuracy: 31.748\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "data_model5 = data_model3\n",
    "X = data_model5.drop('Adjusted_BoxOffice_Cat',axis = 1)\n",
    "y = data_model5['Adjusted_BoxOffice_Cat']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state = 1)\n",
    "\n",
    "for k in range(1,601,100):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(neigh, X_train, y_train, cv=5)\n",
    "\n",
    "    neigh.fit(X_train, y_train)\n",
    "    y_pred = neigh.predict(X_test)\n",
    "    tst_acc = accuracy_score(y_test, y_pred) * 100\n",
    "    cv_acc = np.mean(scores) * 100\n",
    "    print('KNN: CV Accuracy: %0.3f | Testing Accuracy: %0.3f' % (cv_acc,tst_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF: CV Accuracy: 32.614 | Testing Accuracy: 32.249\n",
      "RF: CV Accuracy: 34.207 | Testing Accuracy: 35.753\n",
      "RF: CV Accuracy: 35.755 | Testing Accuracy: 36.682\n",
      "RF: CV Accuracy: 36.765 | Testing Accuracy: 37.504\n",
      "RF: CV Accuracy: 36.552 | Testing Accuracy: 38.970\n",
      "RF: CV Accuracy: 36.780 | Testing Accuracy: 37.969\n",
      "RF: CV Accuracy: 36.059 | Testing Accuracy: 37.934\n",
      "RF: CV Accuracy: 35.799 | Testing Accuracy: 36.932\n",
      "RF: CV Accuracy: 34.160 | Testing Accuracy: 35.467\n",
      "RF: CV Accuracy: 33.899 | Testing Accuracy: 34.537\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "\n",
    "data_model6 = data_model3\n",
    "X = data_model6.drop('Adjusted_BoxOffice_Cat',axis = 1)\n",
    "y = data_model6['Adjusted_BoxOffice_Cat']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state = 1)\n",
    "for dep in range(2, 22,2):\n",
    "    rf_clf = RandomForestClassifier(n_estimators=100, max_depth=dep)\n",
    "    scores = cross_val_score(rf_clf, X_train, y_train, cv=5)\n",
    "\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    y_pred = rf_clf.predict(X_test)\n",
    "    tst_acc = accuracy_score(y_test, y_pred) * 100\n",
    "    cv_acc = np.mean(scores) * 100\n",
    "    print('RF: CV Accuracy: %0.3f | Testing Accuracy: %0.3f' % (cv_acc,tst_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF: CV Accuracy: 35.140 | Testing Accuracy: 36.718\n",
      "RF: CV Accuracy: 36.751 | Testing Accuracy: 38.184\n",
      "RF: CV Accuracy: 36.566 | Testing Accuracy: 37.826\n",
      "RF: CV Accuracy: 36.827 | Testing Accuracy: 38.827\n",
      "RF: CV Accuracy: 36.858 | Testing Accuracy: 38.148\n",
      "RF: CV Accuracy: 36.858 | Testing Accuracy: 38.613\n",
      "RF: CV Accuracy: 36.905 | Testing Accuracy: 38.112\n",
      "RF: CV Accuracy: 36.950 | Testing Accuracy: 37.969\n",
      "RF: CV Accuracy: 37.149 | Testing Accuracy: 38.112\n",
      "RF: CV Accuracy: 37.103 | Testing Accuracy: 38.220\n"
     ]
    }
   ],
   "source": [
    "for num in range(10, 210,20):\n",
    "    rf_clf = RandomForestClassifier(n_estimators=num, max_depth=10)\n",
    "    scores = cross_val_score(rf_clf, X_train, y_train, cv=5)\n",
    "\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    y_pred = rf_clf.predict(X_test)\n",
    "    tst_acc = accuracy_score(y_test, y_pred) * 100\n",
    "    cv_acc = np.mean(scores) * 100\n",
    "    print('RF: CV Accuracy: %0.3f | Testing Accuracy: %0.3f' % (cv_acc,tst_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF: CV Accuracy: 36.613 | Testing Accuracy: 37.862\n",
      "RF: CV Accuracy: 36.751 | Testing Accuracy: 38.148\n",
      "RF: CV Accuracy: 37.042 | Testing Accuracy: 37.934\n",
      "RF: CV Accuracy: 36.721 | Testing Accuracy: 38.363\n",
      "RF: CV Accuracy: 36.750 | Testing Accuracy: 37.969\n",
      "RF: CV Accuracy: 36.337 | Testing Accuracy: 38.363\n",
      "RF: CV Accuracy: 36.827 | Testing Accuracy: 38.398\n",
      "RF: CV Accuracy: 36.767 | Testing Accuracy: 38.291\n",
      "RF: CV Accuracy: 36.614 | Testing Accuracy: 38.184\n",
      "RF: CV Accuracy: 37.087 | Testing Accuracy: 38.005\n"
     ]
    }
   ],
   "source": [
    "for mini in range(2, 22,2):\n",
    "    rf_clf = RandomForestClassifier(n_estimators=70, max_depth=10, min_samples_split = mini)\n",
    "    scores = cross_val_score(rf_clf, X_train, y_train, cv=5)\n",
    "\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    y_pred = rf_clf.predict(X_test)\n",
    "    tst_acc = accuracy_score(y_test, y_pred) * 100\n",
    "    cv_acc = np.mean(scores) * 100\n",
    "    print('RF: CV Accuracy: %0.3f | Testing Accuracy: %0.3f' % (cv_acc,tst_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF: CV Accuracy: 36.720 | Testing Accuracy: 37.075\n",
      "RF: CV Accuracy: 36.996 | Testing Accuracy: 38.792\n",
      "RF: CV Accuracy: 36.720 | Testing Accuracy: 38.077\n",
      "RF: CV Accuracy: 37.072 | Testing Accuracy: 38.470\n",
      "RF: CV Accuracy: 37.041 | Testing Accuracy: 38.005\n",
      "RF: CV Accuracy: 36.934 | Testing Accuracy: 38.541\n",
      "RF: CV Accuracy: 37.103 | Testing Accuracy: 38.112\n",
      "RF: CV Accuracy: 36.826 | Testing Accuracy: 38.077\n",
      "RF: CV Accuracy: 36.919 | Testing Accuracy: 37.969\n",
      "RF: CV Accuracy: 36.996 | Testing Accuracy: 38.112\n",
      "RF: CV Accuracy: 37.134 | Testing Accuracy: 38.470\n",
      "RF: CV Accuracy: 37.332 | Testing Accuracy: 38.684\n",
      "RF: CV Accuracy: 36.397 | Testing Accuracy: 38.756\n",
      "RF: CV Accuracy: 36.995 | Testing Accuracy: 38.291\n",
      "RF: CV Accuracy: 36.734 | Testing Accuracy: 38.649\n",
      "RF: CV Accuracy: 37.241 | Testing Accuracy: 38.899\n",
      "RF: CV Accuracy: 37.317 | Testing Accuracy: 37.934\n",
      "RF: CV Accuracy: 37.118 | Testing Accuracy: 38.470\n",
      "RF: CV Accuracy: 36.567 | Testing Accuracy: 38.077\n"
     ]
    }
   ],
   "source": [
    "for leaf in range(50, 1000,50):\n",
    "    rf_clf = RandomForestClassifier(n_estimators=70, max_depth=10, min_samples_split = 14, max_leaf_nodes = leaf)\n",
    "    scores = cross_val_score(rf_clf, X_train, y_train, cv=5)\n",
    "\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    y_pred = rf_clf.predict(X_test)\n",
    "    tst_acc = accuracy_score(y_test, y_pred) * 100\n",
    "    cv_acc = np.mean(scores) * 100\n",
    "    print('RF: CV Accuracy: %0.3f | Testing Accuracy: %0.3f' % (cv_acc,tst_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB: CV Accuracy: 37.364 | Testing Accuracy: 35.753\n",
      "GB: CV Accuracy: 37.149 | Testing Accuracy: 37.790\n",
      "GB: CV Accuracy: 36.796 | Testing Accuracy: 34.787\n",
      "GB: CV Accuracy: 36.766 | Testing Accuracy: 33.107\n"
     ]
    }
   ],
   "source": [
    "# GB\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "data_model10 = data_model3\n",
    "X = data_model10.drop('Adjusted_BoxOffice_Cat',axis = 1)\n",
    "y = data_model10['Adjusted_BoxOffice_Cat']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state = 1)\n",
    "for lr in [0.01, 0.1, 0.4, 0.618]:\n",
    "    gb_clf = GradientBoostingClassifier(learning_rate=lr)\n",
    "    scores = cross_val_score(rf_clf, X_train, y_train, cv=5)\n",
    "\n",
    "    gb_clf.fit(X_train, y_train)\n",
    "    y_pred = gb_clf.predict(X_test)\n",
    "    tst_acc = accuracy_score(y_test, y_pred) * 100\n",
    "    cv_acc = np.mean(scores) * 100\n",
    "    print('GB: CV Accuracy: %0.3f | Testing Accuracy: %0.3f' % (cv_acc,tst_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB: CV Accuracy: 37.164 | Testing Accuracy: 35.788\n",
      "GB: CV Accuracy: 36.751 | Testing Accuracy: 37.361\n",
      "GB: CV Accuracy: 37.149 | Testing Accuracy: 37.755\n",
      "GB: CV Accuracy: 36.644 | Testing Accuracy: 37.290\n",
      "GB: CV Accuracy: 36.490 | Testing Accuracy: 35.395\n"
     ]
    }
   ],
   "source": [
    "# GB\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "data_model10 = data_model3\n",
    "X = data_model10.drop('Adjusted_BoxOffice_Cat',axis = 1)\n",
    "y = data_model10['Adjusted_BoxOffice_Cat']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state = 1)\n",
    "for num in [10, 50, 100, 200, 500]:\n",
    "    gb_clf = GradientBoostingClassifier(n_estimators=num, learning_rate=0.1)\n",
    "    scores = cross_val_score(rf_clf, X_train, y_train, cv=5)\n",
    "\n",
    "    gb_clf.fit(X_train, y_train)\n",
    "    y_pred = gb_clf.predict(X_test)\n",
    "    tst_acc = accuracy_score(y_test, y_pred) * 100\n",
    "    cv_acc = np.mean(scores) * 100\n",
    "    print('GB: CV Accuracy: %0.3f | Testing Accuracy: %0.3f' % (cv_acc,tst_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB: CV Accuracy: 36.613 | Testing Accuracy: 37.898\n",
      "GB: CV Accuracy: 37.410 | Testing Accuracy: 38.077\n",
      "GB: CV Accuracy: 37.056 | Testing Accuracy: 38.220\n",
      "GB: CV Accuracy: 37.502 | Testing Accuracy: 37.826\n",
      "GB: CV Accuracy: 37.042 | Testing Accuracy: 37.647\n"
     ]
    }
   ],
   "source": [
    "# GB\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "data_model10 = data_model3\n",
    "X = data_model10.drop('Adjusted_BoxOffice_Cat',axis = 1)\n",
    "y = data_model10['Adjusted_BoxOffice_Cat']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state = 1)\n",
    "for mini in [2, 10, 20, 50, 100]:\n",
    "    gb_clf = GradientBoostingClassifier(min_samples_split = mini, n_estimators=100, learning_rate=0.1)\n",
    "    scores = cross_val_score(rf_clf, X_train, y_train, cv=5)\n",
    "\n",
    "    gb_clf.fit(X_train, y_train)\n",
    "    y_pred = gb_clf.predict(X_test)\n",
    "    tst_acc = accuracy_score(y_test, y_pred) * 100\n",
    "    cv_acc = np.mean(scores) * 100\n",
    "    print('GB: CV Accuracy: %0.3f | Testing Accuracy: %0.3f' % (cv_acc,tst_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB: CV Accuracy: 37.149 | Testing Accuracy: 38.470\n",
      "GB: CV Accuracy: 37.010 | Testing Accuracy: 36.789\n",
      "GB: CV Accuracy: 36.934 | Testing Accuracy: 33.750\n",
      "GB: CV Accuracy: 36.627 | Testing Accuracy: 32.749\n",
      "GB: CV Accuracy: 37.593 | Testing Accuracy: 32.177\n"
     ]
    }
   ],
   "source": [
    "# GB\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "data_model10 = data_model3\n",
    "X = data_model10.drop('Adjusted_BoxOffice_Cat',axis = 1)\n",
    "y = data_model10['Adjusted_BoxOffice_Cat']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state = 1)\n",
    "for dep in [2, 5, 10, 20, 50]:\n",
    "    gb_clf = GradientBoostingClassifier(max_depth = dep, min_samples_split = 20, n_estimators=100, learning_rate=0.1)\n",
    "    scores = cross_val_score(rf_clf, X_train, y_train, cv=5)\n",
    "\n",
    "    gb_clf.fit(X_train, y_train)\n",
    "    y_pred = gb_clf.predict(X_test)\n",
    "    tst_acc = accuracy_score(y_test, y_pred) * 100\n",
    "    cv_acc = np.mean(scores) * 100\n",
    "    print('GB: CV Accuracy: %0.3f | Testing Accuracy: %0.3f' % (cv_acc,tst_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6525/6525 [==============================] - 1s 114us/step - loss: 2.0246 - acc: 0.2025\n",
      "Epoch 2/100\n",
      "6525/6525 [==============================] - 0s 53us/step - loss: 1.7877 - acc: 0.3027\n",
      "Epoch 3/100\n",
      "6525/6525 [==============================] - 0s 52us/step - loss: 1.6687 - acc: 0.3309\n",
      "Epoch 4/100\n",
      "6525/6525 [==============================] - 0s 54us/step - loss: 1.5972 - acc: 0.3476\n",
      "Epoch 5/100\n",
      "6525/6525 [==============================] - 0s 53us/step - loss: 1.5591 - acc: 0.3534\n",
      "Epoch 6/100\n",
      "6525/6525 [==============================] - 0s 54us/step - loss: 1.5386 - acc: 0.3603\n",
      "Epoch 7/100\n",
      "6525/6525 [==============================] - 0s 54us/step - loss: 1.5248 - acc: 0.3674\n",
      "Epoch 8/100\n",
      "6525/6525 [==============================] - 0s 53us/step - loss: 1.5161 - acc: 0.3726\n",
      "Epoch 9/100\n",
      "6525/6525 [==============================] - 0s 53us/step - loss: 1.5083 - acc: 0.3752\n",
      "Epoch 10/100\n",
      "6525/6525 [==============================] - 0s 52us/step - loss: 1.5030 - acc: 0.3810\n",
      "Epoch 11/100\n",
      "6525/6525 [==============================] - 0s 55us/step - loss: 1.4980 - acc: 0.3744\n",
      "Epoch 12/100\n",
      "6525/6525 [==============================] - 0s 55us/step - loss: 1.4937 - acc: 0.3796\n",
      "Epoch 13/100\n",
      "6525/6525 [==============================] - 0s 54us/step - loss: 1.4890 - acc: 0.3825\n",
      "Epoch 14/100\n",
      "6525/6525 [==============================] - 0s 54us/step - loss: 1.4870 - acc: 0.3842\n",
      "Epoch 15/100\n",
      "6525/6525 [==============================] - 0s 52us/step - loss: 1.4834 - acc: 0.3838\n",
      "Epoch 16/100\n",
      "6525/6525 [==============================] - 0s 52us/step - loss: 1.4808 - acc: 0.3838\n",
      "Epoch 17/100\n",
      "6525/6525 [==============================] - 0s 57us/step - loss: 1.4792 - acc: 0.3867\n",
      "Epoch 18/100\n",
      "6525/6525 [==============================] - 0s 57us/step - loss: 1.4761 - acc: 0.3876\n",
      "Epoch 19/100\n",
      "6525/6525 [==============================] - 0s 53us/step - loss: 1.4741 - acc: 0.3857\n",
      "Epoch 20/100\n",
      "6525/6525 [==============================] - 0s 53us/step - loss: 1.4721 - acc: 0.3877\n",
      "Epoch 21/100\n",
      "6525/6525 [==============================] - 0s 69us/step - loss: 1.4698 - acc: 0.3865\n",
      "Epoch 22/100\n",
      "6525/6525 [==============================] - 0s 76us/step - loss: 1.4681 - acc: 0.3919\n",
      "Epoch 23/100\n",
      "6525/6525 [==============================] - 0s 72us/step - loss: 1.4661 - acc: 0.3928\n",
      "Epoch 24/100\n",
      "6525/6525 [==============================] - 1s 89us/step - loss: 1.4634 - acc: 0.3954\n",
      "Epoch 25/100\n",
      "6525/6525 [==============================] - 1s 97us/step - loss: 1.4624 - acc: 0.3940\n",
      "Epoch 26/100\n",
      "6525/6525 [==============================] - 1s 88us/step - loss: 1.4607 - acc: 0.3945\n",
      "Epoch 27/100\n",
      "6525/6525 [==============================] - 0s 70us/step - loss: 1.4590 - acc: 0.3966\n",
      "Epoch 28/100\n",
      "6525/6525 [==============================] - 1s 120us/step - loss: 1.4578 - acc: 0.3985\n",
      "Epoch 29/100\n",
      "6525/6525 [==============================] - 0s 67us/step - loss: 1.4562 - acc: 0.3997\n",
      "Epoch 30/100\n",
      "6525/6525 [==============================] - 1s 117us/step - loss: 1.4545 - acc: 0.3982\n",
      "Epoch 31/100\n",
      "6525/6525 [==============================] - 0s 66us/step - loss: 1.4533 - acc: 0.3982\n",
      "Epoch 32/100\n",
      "6525/6525 [==============================] - 1s 83us/step - loss: 1.4522 - acc: 0.3963\n",
      "Epoch 33/100\n",
      "6525/6525 [==============================] - 1s 113us/step - loss: 1.4505 - acc: 0.4029\n",
      "Epoch 34/100\n",
      "6525/6525 [==============================] - 1s 110us/step - loss: 1.4490 - acc: 0.4018\n",
      "Epoch 35/100\n",
      "6525/6525 [==============================] - 0s 62us/step - loss: 1.4476 - acc: 0.4031\n",
      "Epoch 36/100\n",
      "6525/6525 [==============================] - 1s 89us/step - loss: 1.4466 - acc: 0.4048\n",
      "Epoch 37/100\n",
      "6525/6525 [==============================] - 1s 90us/step - loss: 1.4455 - acc: 0.4044\n",
      "Epoch 38/100\n",
      "6525/6525 [==============================] - 1s 85us/step - loss: 1.4440 - acc: 0.4040\n",
      "Epoch 39/100\n",
      "6525/6525 [==============================] - 1s 82us/step - loss: 1.4439 - acc: 0.3983\n",
      "Epoch 40/100\n",
      "6525/6525 [==============================] - 0s 55us/step - loss: 1.4424 - acc: 0.4044\n",
      "Epoch 41/100\n",
      "6525/6525 [==============================] - 0s 55us/step - loss: 1.4418 - acc: 0.4020\n",
      "Epoch 42/100\n",
      "6525/6525 [==============================] - 0s 54us/step - loss: 1.4413 - acc: 0.4048\n",
      "Epoch 43/100\n",
      "6525/6525 [==============================] - 0s 53us/step - loss: 1.4397 - acc: 0.4046\n",
      "Epoch 44/100\n",
      "6525/6525 [==============================] - 0s 56us/step - loss: 1.4394 - acc: 0.4067\n",
      "Epoch 45/100\n",
      "6525/6525 [==============================] - 0s 54us/step - loss: 1.4382 - acc: 0.4070\n",
      "Epoch 46/100\n",
      "6525/6525 [==============================] - 0s 60us/step - loss: 1.4374 - acc: 0.4077\n",
      "Epoch 47/100\n",
      "6525/6525 [==============================] - 0s 54us/step - loss: 1.4371 - acc: 0.4072\n",
      "Epoch 48/100\n",
      "6525/6525 [==============================] - 0s 53us/step - loss: 1.4363 - acc: 0.4064\n",
      "Epoch 49/100\n",
      "6525/6525 [==============================] - 0s 53us/step - loss: 1.4343 - acc: 0.4048\n",
      "Epoch 50/100\n",
      "6525/6525 [==============================] - 0s 55us/step - loss: 1.4341 - acc: 0.4135\n",
      "Epoch 51/100\n",
      "6525/6525 [==============================] - 0s 55us/step - loss: 1.4328 - acc: 0.4110\n",
      "Epoch 52/100\n",
      "6525/6525 [==============================] - 0s 52us/step - loss: 1.4318 - acc: 0.4116\n",
      "Epoch 53/100\n",
      "6525/6525 [==============================] - 0s 55us/step - loss: 1.4313 - acc: 0.4121\n",
      "Epoch 54/100\n",
      "6525/6525 [==============================] - 0s 57us/step - loss: 1.4300 - acc: 0.4113\n",
      "Epoch 55/100\n",
      "6525/6525 [==============================] - 0s 53us/step - loss: 1.4294 - acc: 0.4162\n",
      "Epoch 56/100\n",
      "6525/6525 [==============================] - 0s 52us/step - loss: 1.4290 - acc: 0.4124\n",
      "Epoch 57/100\n",
      "6525/6525 [==============================] - 0s 53us/step - loss: 1.4282 - acc: 0.4126\n",
      "Epoch 58/100\n",
      "6525/6525 [==============================] - 0s 54us/step - loss: 1.4272 - acc: 0.4118\n",
      "Epoch 59/100\n",
      "6525/6525 [==============================] - 0s 55us/step - loss: 1.4265 - acc: 0.4139\n",
      "Epoch 60/100\n",
      "6525/6525 [==============================] - 0s 54us/step - loss: 1.4267 - acc: 0.4141\n",
      "Epoch 61/100\n",
      "6525/6525 [==============================] - 0s 53us/step - loss: 1.4257 - acc: 0.4126\n",
      "Epoch 62/100\n",
      "6525/6525 [==============================] - 0s 57us/step - loss: 1.4240 - acc: 0.4162\n",
      "Epoch 63/100\n",
      "6525/6525 [==============================] - 0s 56us/step - loss: 1.4248 - acc: 0.4092\n",
      "Epoch 64/100\n",
      "6525/6525 [==============================] - 0s 54us/step - loss: 1.4233 - acc: 0.4161\n",
      "Epoch 65/100\n",
      "6525/6525 [==============================] - 0s 54us/step - loss: 1.4216 - acc: 0.4162\n",
      "Epoch 66/100\n",
      "6525/6525 [==============================] - 0s 58us/step - loss: 1.4229 - acc: 0.4169\n",
      "Epoch 67/100\n",
      "6525/6525 [==============================] - 0s 54us/step - loss: 1.4220 - acc: 0.4130\n",
      "Epoch 68/100\n",
      "6525/6525 [==============================] - 0s 56us/step - loss: 1.4209 - acc: 0.4162\n",
      "Epoch 69/100\n",
      "6525/6525 [==============================] - 0s 54us/step - loss: 1.4206 - acc: 0.4164\n",
      "Epoch 70/100\n",
      "6525/6525 [==============================] - 0s 54us/step - loss: 1.4202 - acc: 0.4132\n",
      "Epoch 71/100\n",
      "6525/6525 [==============================] - 0s 54us/step - loss: 1.4196 - acc: 0.4147\n",
      "Epoch 72/100\n",
      "6525/6525 [==============================] - 0s 53us/step - loss: 1.4187 - acc: 0.4181\n",
      "Epoch 73/100\n",
      "6525/6525 [==============================] - 0s 61us/step - loss: 1.4188 - acc: 0.4201\n",
      "Epoch 74/100\n",
      "6525/6525 [==============================] - 0s 76us/step - loss: 1.4177 - acc: 0.4176\n",
      "Epoch 75/100\n",
      "6525/6525 [==============================] - 0s 65us/step - loss: 1.4171 - acc: 0.4110\n",
      "Epoch 76/100\n",
      "6525/6525 [==============================] - 0s 74us/step - loss: 1.4171 - acc: 0.4175\n",
      "Epoch 77/100\n",
      "6525/6525 [==============================] - 0s 56us/step - loss: 1.4157 - acc: 0.4176\n",
      "Epoch 78/100\n",
      "6525/6525 [==============================] - 0s 56us/step - loss: 1.4162 - acc: 0.4182\n",
      "Epoch 79/100\n",
      "6525/6525 [==============================] - 0s 57us/step - loss: 1.4152 - acc: 0.4166\n",
      "Epoch 80/100\n",
      "6525/6525 [==============================] - 0s 68us/step - loss: 1.4145 - acc: 0.4201\n",
      "Epoch 81/100\n",
      "6525/6525 [==============================] - 0s 66us/step - loss: 1.4139 - acc: 0.4187\n",
      "Epoch 82/100\n",
      "6525/6525 [==============================] - 0s 50us/step - loss: 1.4138 - acc: 0.4193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "6525/6525 [==============================] - 0s 48us/step - loss: 1.4143 - acc: 0.4192\n",
      "Epoch 84/100\n",
      "6525/6525 [==============================] - 0s 54us/step - loss: 1.4128 - acc: 0.4166\n",
      "Epoch 85/100\n",
      "6525/6525 [==============================] - 0s 57us/step - loss: 1.4133 - acc: 0.4184\n",
      "Epoch 86/100\n",
      "6525/6525 [==============================] - 0s 61us/step - loss: 1.4119 - acc: 0.4176\n",
      "Epoch 87/100\n",
      "6525/6525 [==============================] - 0s 53us/step - loss: 1.4121 - acc: 0.4175\n",
      "Epoch 88/100\n",
      "6525/6525 [==============================] - 0s 62us/step - loss: 1.4114 - acc: 0.4192\n",
      "Epoch 89/100\n",
      "6525/6525 [==============================] - 0s 52us/step - loss: 1.4115 - acc: 0.4185\n",
      "Epoch 90/100\n",
      "6525/6525 [==============================] - 0s 59us/step - loss: 1.4104 - acc: 0.4170\n",
      "Epoch 91/100\n",
      "6525/6525 [==============================] - 0s 61us/step - loss: 1.4099 - acc: 0.4182\n",
      "Epoch 92/100\n",
      "6525/6525 [==============================] - 0s 68us/step - loss: 1.4100 - acc: 0.4179\n",
      "Epoch 93/100\n",
      "6525/6525 [==============================] - 0s 71us/step - loss: 1.4094 - acc: 0.4213\n",
      "Epoch 94/100\n",
      "6525/6525 [==============================] - 0s 43us/step - loss: 1.4086 - acc: 0.4210\n",
      "Epoch 95/100\n",
      "6525/6525 [==============================] - 0s 51us/step - loss: 1.4081 - acc: 0.4185\n",
      "Epoch 96/100\n",
      "6525/6525 [==============================] - 0s 45us/step - loss: 1.4079 - acc: 0.4210\n",
      "Epoch 97/100\n",
      "6525/6525 [==============================] - 0s 60us/step - loss: 1.4079 - acc: 0.4201\n",
      "Epoch 98/100\n",
      "6525/6525 [==============================] - 0s 42us/step - loss: 1.4076 - acc: 0.4225\n",
      "Epoch 99/100\n",
      "6525/6525 [==============================] - 0s 50us/step - loss: 1.4071 - acc: 0.4219\n",
      "Epoch 100/100\n",
      "6525/6525 [==============================] - 0s 53us/step - loss: 1.4064 - acc: 0.4259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.37397211297819094"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot_encoding(y):\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y)\n",
    "    encoded_y = encoder.transform(y)\n",
    "    dummy_y = np_utils.to_categorical(encoded_y)\n",
    "    return (dummy_y)\n",
    "\n",
    "data_nn = data[['Adjusted_BoxOffice_Cat', 'Runtime',\n",
    "        'Action','Adventure', 'Animation', 'Biography', 'Comedy', 'Crime', 'Documentary',\n",
    "       'Drama', 'Family', 'Fantasy', 'Film-Noir', 'History', 'Horror', 'Music',\n",
    "       'Musical', 'Mystery', 'News', 'Romance', 'Sci-Fi', 'Short', 'Sport',\n",
    "       'Thriller', 'War', 'Western', 'Country_count','NLP_Score']]\n",
    "\n",
    "\n",
    "X = data_nn.drop('Adjusted_BoxOffice_Cat',axis = 1)\n",
    "y = data_nn['Adjusted_BoxOffice_Cat']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state = 1)\n",
    "\n",
    "dummy_y_train = one_hot_encoding(y_train)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=data_nn.shape[1]-1, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(len(all_cats), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, dummy_y_train, batch_size = 10, epochs = 100)\n",
    "\n",
    "predictions = model.predict_classes(X_test)\n",
    "prediction_ = np.argmax(np_utils.to_categorical(predictions), axis = 1)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7457 samples, validate on 1865 samples\n",
      "Epoch 1/400\n",
      "7457/7457 [==============================] - 2s 318us/step - loss: 2.4086 - acc: 0.2638 - val_loss: 2.1019 - val_acc: 0.3657\n",
      "Epoch 2/400\n",
      "7457/7457 [==============================] - 1s 88us/step - loss: 2.0510 - acc: 0.3284 - val_loss: 1.9323 - val_acc: 0.3689\n",
      "Epoch 3/400\n",
      "7457/7457 [==============================] - 1s 97us/step - loss: 1.9089 - acc: 0.3424 - val_loss: 1.8242 - val_acc: 0.3786\n",
      "Epoch 4/400\n",
      "7457/7457 [==============================] - 1s 100us/step - loss: 1.8138 - acc: 0.3518 - val_loss: 1.7688 - val_acc: 0.3769\n",
      "Epoch 5/400\n",
      "7457/7457 [==============================] - 1s 94us/step - loss: 1.7514 - acc: 0.3617 - val_loss: 1.7177 - val_acc: 0.3748\n",
      "Epoch 6/400\n",
      "7457/7457 [==============================] - 1s 93us/step - loss: 1.7026 - acc: 0.3586 - val_loss: 1.6846 - val_acc: 0.3796\n",
      "Epoch 7/400\n",
      "7457/7457 [==============================] - 1s 98us/step - loss: 1.6712 - acc: 0.3646 - val_loss: 1.6642 - val_acc: 0.3871\n",
      "Epoch 8/400\n",
      "7457/7457 [==============================] - 1s 96us/step - loss: 1.6449 - acc: 0.3727 - val_loss: 1.6539 - val_acc: 0.3710\n",
      "Epoch 9/400\n",
      "7457/7457 [==============================] - 1s 94us/step - loss: 1.6210 - acc: 0.3782 - val_loss: 1.6392 - val_acc: 0.3764\n",
      "Epoch 10/400\n",
      "7457/7457 [==============================] - 1s 88us/step - loss: 1.6145 - acc: 0.3668 - val_loss: 1.6345 - val_acc: 0.3796\n",
      "Epoch 11/400\n",
      "7457/7457 [==============================] - 1s 91us/step - loss: 1.5971 - acc: 0.3771 - val_loss: 1.6142 - val_acc: 0.3845\n",
      "Epoch 12/400\n",
      "7457/7457 [==============================] - 1s 93us/step - loss: 1.5899 - acc: 0.3772 - val_loss: 1.6173 - val_acc: 0.3802\n",
      "Epoch 13/400\n",
      "7457/7457 [==============================] - 1s 91us/step - loss: 1.5759 - acc: 0.3751 - val_loss: 1.6212 - val_acc: 0.3812\n",
      "Epoch 14/400\n",
      "7457/7457 [==============================] - 1s 91us/step - loss: 1.5754 - acc: 0.3831 - val_loss: 1.6091 - val_acc: 0.3759\n",
      "Epoch 15/400\n",
      "7457/7457 [==============================] - 1s 87us/step - loss: 1.5646 - acc: 0.3873 - val_loss: 1.6057 - val_acc: 0.3845\n",
      "Epoch 16/400\n",
      "7457/7457 [==============================] - 1s 87us/step - loss: 1.5617 - acc: 0.3759 - val_loss: 1.5977 - val_acc: 0.3802\n",
      "Epoch 17/400\n",
      "7457/7457 [==============================] - 1s 87us/step - loss: 1.5668 - acc: 0.3830 - val_loss: 1.5873 - val_acc: 0.3823\n",
      "Epoch 18/400\n",
      "7457/7457 [==============================] - 1s 88us/step - loss: 1.5518 - acc: 0.3842 - val_loss: 1.6048 - val_acc: 0.3743\n",
      "Epoch 19/400\n",
      "7457/7457 [==============================] - 1s 87us/step - loss: 1.5547 - acc: 0.3936 - val_loss: 1.5937 - val_acc: 0.3759\n",
      "Epoch 20/400\n",
      "7457/7457 [==============================] - 1s 87us/step - loss: 1.5557 - acc: 0.3885 - val_loss: 1.5930 - val_acc: 0.3802\n",
      "Epoch 21/400\n",
      "7457/7457 [==============================] - 1s 87us/step - loss: 1.5472 - acc: 0.3850 - val_loss: 1.5997 - val_acc: 0.3769\n",
      "Epoch 22/400\n",
      "7457/7457 [==============================] - 1s 87us/step - loss: 1.5401 - acc: 0.3936 - val_loss: 1.5982 - val_acc: 0.3748\n",
      "Epoch 23/400\n",
      "7457/7457 [==============================] - 1s 87us/step - loss: 1.5461 - acc: 0.3893 - val_loss: 1.5872 - val_acc: 0.3807\n",
      "Epoch 24/400\n",
      "7457/7457 [==============================] - 1s 87us/step - loss: 1.5457 - acc: 0.3851 - val_loss: 1.5939 - val_acc: 0.3818\n",
      "Epoch 25/400\n",
      "7457/7457 [==============================] - 1s 88us/step - loss: 1.5420 - acc: 0.3857 - val_loss: 1.5935 - val_acc: 0.3710\n",
      "Epoch 26/400\n",
      "7457/7457 [==============================] - 1s 88us/step - loss: 1.5375 - acc: 0.3835 - val_loss: 1.5856 - val_acc: 0.3743\n",
      "Epoch 27/400\n",
      "7457/7457 [==============================] - 1s 88us/step - loss: 1.5378 - acc: 0.3884 - val_loss: 1.5899 - val_acc: 0.3818\n",
      "Epoch 28/400\n",
      "7457/7457 [==============================] - 1s 87us/step - loss: 1.5323 - acc: 0.3890 - val_loss: 1.5938 - val_acc: 0.3753\n",
      "Epoch 29/400\n",
      "7457/7457 [==============================] - 1s 88us/step - loss: 1.5345 - acc: 0.3826 - val_loss: 1.5992 - val_acc: 0.3845\n",
      "Epoch 30/400\n",
      "7457/7457 [==============================] - 1s 88us/step - loss: 1.5237 - acc: 0.3936 - val_loss: 1.5943 - val_acc: 0.3861\n",
      "Epoch 31/400\n",
      "7457/7457 [==============================] - 1s 88us/step - loss: 1.5253 - acc: 0.3798 - val_loss: 1.5913 - val_acc: 0.3780\n",
      "Epoch 32/400\n",
      "7457/7457 [==============================] - 1s 87us/step - loss: 1.5240 - acc: 0.3918 - val_loss: 1.5948 - val_acc: 0.3850\n",
      "Epoch 33/400\n",
      "7457/7457 [==============================] - 1s 88us/step - loss: 1.5266 - acc: 0.3851 - val_loss: 1.5867 - val_acc: 0.3791\n",
      "Epoch 34/400\n",
      "7457/7457 [==============================] - 1s 88us/step - loss: 1.5260 - acc: 0.3900 - val_loss: 1.5917 - val_acc: 0.3743\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "def one_hot_encoding(y):\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y)\n",
    "    encoded_y = encoder.transform(y)\n",
    "    dummy_y = np_utils.to_categorical(encoded_y)\n",
    "    return (dummy_y)\n",
    "\n",
    "data_nn = data[['Adjusted_BoxOffice_Cat', 'Runtime',\n",
    "        'Action','Adventure', 'Animation', 'Biography', 'Comedy', 'Crime', 'Documentary',\n",
    "       'Drama', 'Family', 'Fantasy', 'Film-Noir', 'History', 'Horror', 'Music',\n",
    "       'Musical', 'Mystery', 'News', 'Romance', 'Sci-Fi', 'Short', 'Sport',\n",
    "       'Thriller', 'War', 'Western', 'Country_count','NLP_Score']]\n",
    "\n",
    "\n",
    "X = data_nn.drop('Adjusted_BoxOffice_Cat',axis = 1)\n",
    "y = data_nn['Adjusted_BoxOffice_Cat']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state = 1)\n",
    "\n",
    "dummy_y_train = one_hot_encoding(y_train)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=data_nn.shape[1]-1, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(len(all_cats), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "model.fit(X_train, dummy_y_train, batch_size = 64, epochs = 400, callbacks=callbacks, validation_data=(X_test, one_hot_encoding(y_test)))\n",
    "\n",
    "predictions = model.predict_classes(X_test)\n",
    "prediction_ = np.argmax(np_utils.to_categorical(predictions), axis = 1)\n",
    "\n",
    "#accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
